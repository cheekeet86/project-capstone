{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Header](#Header)\n",
    "- [Import Data](#Import-Data)\n",
    "- [Functions](#Functions)\n",
    "\n",
    "\n",
    "- [Drop Rows](#Drop-Rows)\n",
    "- [Select and Merge Sports](#Select-and-Merge-Sports)\n",
    "- [Select Features](#Select-Features)\n",
    "\n",
    "- [Plot df_model](#Plot-df_model)\n",
    "- [Create Features and Target](#Create-Features-and-Target)\n",
    "- [Handle Imbalanced Data](#Handle-Imbalanced-Data)\n",
    "\n",
    "\n",
    "- [Logistic Regression Model](#Logistic-Regression-Model)\n",
    "- [KNN Model](#KNN-Model)\n",
    "- [DTC Model](#DTC-Model)\n",
    "- [RTC Model](#RTC-Model)\n",
    "- [SVC Model](#SVC-Model)\n",
    "\n",
    "\n",
    "- [Combine Model Predictions](#Combine-Model-Predictions)\n",
    "- [VotingClassifier Model](#VotingClassifier-Model)\n",
    "- [GridSearch Model](#GridSearch-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "#from matplotlib_venn import venn2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pydotplus\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures,LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,r2_score,mean_squared_error,cohen_kappa_score,f1_score,precision_score,recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,VotingClassifier\n",
    "from sklearn.externals.six import StringIO\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "raw_path = '../../data/0_raw/fitrec/' \n",
    "input_path = '../../data/1_input/fitrec/'\n",
    "clean_path = '../../data/2_clean/fitrec/' \n",
    "preprocess_path = '../../data/3_preprocess/fitrec/' \n",
    "output_path = '../../data/4_output/fitrec/'\n",
    "\n",
    "sports_path = '../../data/1_input/sports/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import summary csv\n",
    "\n",
    "#file = 'endomondoHR_proper_summary.csv'\n",
    "#file = 'endomondoHR_proper_dist_spd_summary.csv'\n",
    "file = 'endomondoHR_proper_dist_spd_time_summary.csv'\n",
    "\n",
    "in_path = clean_path + file\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sports.xlsx\n",
    "\n",
    "path = sports_path + 'sports.xlsx'\n",
    "df_sports = pd.read_excel(path)\n",
    "df_sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with abnormal heartrate\n",
    "\n",
    "before = len(df)\n",
    "print('before:',before)\n",
    "\n",
    "cond_1 = df['hr_min'] >= 40\n",
    "cond_2 = df['hr_avg'] >= 50\n",
    "cond_3 = df['hr_max'] >= 60\n",
    "\n",
    "df = df[cond_1 & cond_2 & cond_3]\n",
    "\n",
    "after = len(df)\n",
    "print('after:',after)\n",
    "drop = before - after\n",
    "print('drop:',drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with abnormal workout duration\n",
    "\n",
    "before = len(df)\n",
    "print('before:',before)\n",
    "\n",
    "# time_dur in minutes\n",
    "time_dur_mask = df['time_dur'] < 24 * 60\n",
    "df = df[time_dur_mask]\n",
    "\n",
    "after = len(df)\n",
    "print('after:',after)\n",
    "drop = before - after\n",
    "print('drop:',drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with abnormal altitude\n",
    "\n",
    "before = len(df)\n",
    "print('before:',before)\n",
    "\n",
    "# altitude in metres\n",
    "max_alt_mask = df['alt_max'] <= 4000 # below Mount Kinabalu\n",
    "min_alt_mask = df['alt_min'] >= -30 # 10 storeys underground\n",
    "df = df[max_alt_mask & min_alt_mask]\n",
    "\n",
    "after = len(df)\n",
    "print('after:',after)\n",
    "drop = before - after\n",
    "print('drop:',drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows if speed is nan\n",
    "\n",
    "# before = len(df)\n",
    "# print('before:',before)\n",
    "\n",
    "# df.dropna(subset=['spd_avg'],inplace=True)\n",
    "\n",
    "# after = len(df)\n",
    "# print('after:',after)\n",
    "# drop = before - after\n",
    "# print('drop:',drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and Merge Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of workouts per sport (after dropping rows)\n",
    "\n",
    "df['sport'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only sports with speed_valid = 1\n",
    "\n",
    "valid_mask = df_sports['speed_valid'] == 1\n",
    "valid_sport_list = df_sports[valid_mask]['sport']\n",
    "valid_sport_list = list(valid_sport_list)\n",
    "\n",
    "# overwrite: compare specific sports\n",
    "#valid_sport_list = ['kayaking','rowing']\n",
    "\n",
    "valid_mask_2 = df['sport'].isin(valid_sport_list)\n",
    "df = df[valid_mask_2]\n",
    "df['sport'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge similar sports\n",
    "\n",
    "for idx,row in df_sports.iterrows():\n",
    "    \n",
    "    sport = row['sport'].rstrip()\n",
    "    sport_rename = row['sport_rename']\n",
    "    print(sport,sport_rename)\n",
    "    \n",
    "    df['sport'].replace(sport,sport_rename,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of workouts per sport (after selecting and merging sports)\n",
    "\n",
    "df['sport'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection (select columns for df_model)\n",
    "\n",
    "# cols = ['sport','time_dur','alt_avg','alt_min','alt_25','alt_75','alt_max','hr_avg', 'hr_min','hr_25','hr_75','hr_max']\n",
    "\n",
    "#cols = ['sport','hr_avg','hr_min','hr_25','hr_75','hr_max']\n",
    "#cols = ['sport','hr_avg','hr_min','hr_05','hr_25','hr_75','hr_95','hr_max']\n",
    "#cols = ['sport','hr_avg','hr_min','hr_25','hr_75','hr_max','spd_avg']\n",
    "#cols = ['sport','hr_avg','hr_min','hr_25','hr_75','hr_max','spd_avg','spd_min','spd_25','spd_75','spd_max']\n",
    "\n",
    "#cols = ['sport','spd_avg']\n",
    "#cols = ['sport','spd_avg','spd_95']\n",
    "#cols = ['sport','spd_avg','spd_05','spd_25','spd_75','spd_95']\n",
    "\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_95']\n",
    "#cols = ['sport','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_95']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_95']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_25','spd_75']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_min','spd_25','spd_75','spd_max']\n",
    "\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_low','spd_med','spd_high','spd_vhigh']\n",
    "\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_95','alt_avg']\n",
    "#cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_95','alt_avg','alt_diff']\n",
    "cols = ['sport','hr_outof','hr_fatburn','hr_cardio','hr_peak','spd_avg','spd_95','alt_avg','alt_min','alt_max']\n",
    "\n",
    "#cols = ['sport']\n",
    "\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         col_name = 'spd_hr_{}_{}'.format(i,j)\n",
    "#         cols.append(col_name)\n",
    "\n",
    "df_model = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only sports with minimal rows\n",
    "\n",
    "count = df_model['sport'].value_counts()\n",
    "\n",
    "#count_cond = count[count > 1].index\n",
    "#count_cond = count[count >= 5].inde\n",
    "#count_cond = count[count >= 10].index\n",
    "count_cond = count[count >= 50].index\n",
    "#count_cond = count[count >= 70].index\n",
    "#count_cond = count[count >= 100].index\n",
    "#count_cond = count[count >= 200].index\n",
    "#count_cond = count[count >= 800].index\n",
    "#count_cond = count[count >= 1500].index\n",
    "#count_cond = count[count >= 70000].index\n",
    "\n",
    "count_mask = df_model['sport'].isin(count_cond)\n",
    "df_model = df_model[count_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of workout by sports (with minimal rows)\n",
    "\n",
    "df_model['sport'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = df_model.groupby('sport')['hr_max'].median().sort_values(ascending=False).index\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# #plt.xlim(0,300)\n",
    "\n",
    "# sns.boxplot(data=df_model,x='hr_max',y='sport',order=order);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = df_model.groupby('sport')['hr_avg'].median().sort_values(ascending=False).index\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# #plt.xlim(0,300)\n",
    "\n",
    "# sns.boxplot(data=df_model,x='hr_avg',y='sport',order=order);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = df_model.groupby('sport')['hr_min'].median().sort_values(ascending=False).index\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# #plt.xlim(0,300)\n",
    "\n",
    "# sns.boxplot(data=df_model,x='hr_min',y='sport',order=order);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing scaling and encoding before create X and y\n",
    "\n",
    "cols = df_model.columns[1:]\n",
    "\n",
    "ss = StandardScaler()\n",
    "df_model[cols] = ss.fit_transform(df_model[cols])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_model['sport'] = le.fit_transform(df_model['sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and target. next perform train_test_split\n",
    "\n",
    "X = df_model.drop(columns='sport')\n",
    "y = df_model['sport']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3,stratify=y,random_state=3050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check before upsample/downsample\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check before upsample/downsample\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=3050)\n",
    "# X_train, y_train = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get majority class index and row count\n",
    "\n",
    "# sport_counts = df_model['sport'].value_counts()\n",
    "# print(sport_counts)\n",
    "\n",
    "# major_class_index = sport_counts.index[0]\n",
    "# major_class_count = sport_counts.values[0]\n",
    "# print(major_class_index,major_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate our training data back together\n",
    "\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "sport_counts = Xy_train['sport'].value_counts()\n",
    "print(sport_counts)\n",
    "\n",
    "print(Xy_train.shape)\n",
    "Xy_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform upsampling and downsampling\n",
    "\n",
    "sample_size = 12000\n",
    "\n",
    "df_all_sample = pd.DataFrame()\n",
    "\n",
    "sport_list = df_model['sport'].unique()\n",
    "sport_list\n",
    "\n",
    "for sport in sport_list:\n",
    "    \n",
    "    cond = Xy_train['sport'] == sport\n",
    "    df_sport = Xy_train[cond]\n",
    "    \n",
    "    # perform downsampling\n",
    "    if sport_counts[sport] >= sample_size:    \n",
    "        print('downsampling',sport,sport_counts[sport])\n",
    "        df_sample = df_sport.sample(sample_size,replace=False,random_state=3050)\n",
    "        \n",
    "    # perform upsampling\n",
    "    # sport_counts[sport] < sample_size: \n",
    "    else:\n",
    "        print('upsampling',sport,sport_counts[sport])\n",
    "        df_sample = df_sport.sample(sample_size,replace=True,random_state=3050)\n",
    "        \n",
    "    df_all_sample = pd.concat([df_all_sample, df_sample], axis=0)\n",
    "    \n",
    "X_train = df_all_sample.drop(columns='sport').values\n",
    "y_train = df_all_sample['sport'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform upsampling for minority classes\n",
    "\n",
    "# df_all_sample = pd.DataFrame()\n",
    "\n",
    "# sport_list = df_model['sport'].unique()\n",
    "\n",
    "# for sport in sport_list:\n",
    "    \n",
    "#     if sport != major_class_index:\n",
    "        \n",
    "#         cond = df_model['sport'] == sport\n",
    "#         df_sport = df_model[cond]\n",
    "#         #print(sport,len(df_sport))\n",
    "        \n",
    "#         df_sample = df_sport.sample(major_class_count,replace=True,random_state=3050)\n",
    "#         df_all_sample = pd.concat([df_all_sample, df_sample], axis=0)\n",
    "        \n",
    "# cond = df_model['sport'] == major_class_index\n",
    "# df_top = df_model[cond]\n",
    "# df_all_sample = pd.concat([df_all_sample, df_top], axis=0)\n",
    "\n",
    "# X_train = df_all_sample.drop(columns='sport').values\n",
    "# y_train = df_all_sample['sport'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check after upsample/downsample\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check after upsample/downsample\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions in df_pred\n",
    "\n",
    "df_pred = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# perform cross validation\n",
    "score = cross_val_score(logreg,X,y,cv=5)\n",
    "print('score:',score.mean(),score)\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# score model\n",
    "print(\"train r2:\",logreg.score(X_train, y_train))\n",
    "print(\"test r2:\",logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('Logisitic Regression Model')\n",
    "print('rows:actual columns:predicted')\n",
    "print('')\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "df_pred['lr'] = y_pred\n",
    "\n",
    "# y_pred = logreg.predict_proba(X_test)\n",
    "# pd.DataFrame(y_pred,columns=le.classes_)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# perform cross validation\n",
    "score = cross_val_score(knn,X,y,cv=5)\n",
    "print('score:',score.mean(),score)\n",
    "\n",
    "# fit model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# score model\n",
    "print(\"train r2:\",knn.score(X_train, y_train))\n",
    "print(\"test r2:\",knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('KNN Model')\n",
    "print('rows:actual columns:predicted')\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "df_pred['knn'] = y_pred\n",
    "\n",
    "# y_pred = knn.predict_proba(X_test)\n",
    "# pd.DataFrame(y_pred,columns=le.classes_)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "dtc = DecisionTreeClassifier(max_depth=10,random_state=3050)\n",
    "\n",
    "# perform cross validation\n",
    "score = cross_val_score(dtc,X,y,cv=5)\n",
    "print(score.mean(),score)\n",
    "\n",
    "# fit model\n",
    "dtc = dtc.fit(X_train,y_train)\n",
    "\n",
    "# score model\n",
    "print(\"train r2:\",dtc.score(X_train, y_train))\n",
    "print(\"test r2:\",dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('DTC Model')\n",
    "print('rows:actual columns:predicted')\n",
    "\n",
    "y_pred = dtc.predict(X_test)\n",
    "df_pred['dtc'] = y_pred\n",
    "\n",
    "# y_pred = dtc.predict_proba(X_test)\n",
    "# pd.DataFrame(y_pred,columns=le.classes_)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the output file object\n",
    "# dot_data = StringIO() \n",
    "\n",
    "# # my fit DecisionTreeRegressor object here is: dtr1\n",
    "# # for feature_names i put the columns of my Xr matrix\n",
    "# export_graphviz(dtc, \n",
    "#                 out_file=dot_data,  \n",
    "#                 filled=True, \n",
    "#                 rounded=True,\n",
    "#                 special_characters=True,\n",
    "#                 feature_names=df_model[features].columns\n",
    "#                )  \n",
    "\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "rfc = RandomForestClassifier(n_estimators=10,max_depth=10,n_jobs=-1, random_state=3050)\n",
    "\n",
    "# perform cross validation\n",
    "score = cross_val_score(rfc,X,y,cv=5)\n",
    "print(score.mean(),score)\n",
    "\n",
    "# fit model\n",
    "rfc = rfc.fit(X_train,y_train)\n",
    "\n",
    "# score model\n",
    "print(\"train r2:\",rfc.score(X_train, y_train))\n",
    "print(\"test r2:\",rfc.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('RFC Model')\n",
    "print('rows:actual columns:predicted')\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "df_pred['rfc'] = y_pred\n",
    "\n",
    "# y_pred = rfc.predict_proba(X_test)\n",
    "# pd.DataFrame(y_pred,columns=le.classes_)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "svc = SVC()\n",
    "\n",
    "# fit model\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# score model\n",
    "print(\"train r2:\",svc.score(X_train, y_train))\n",
    "print(\"test r2:\",svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('SVC Model')\n",
    "print('rows:actual columns:predicted')\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "df_pred['svc'] = y_pred\n",
    "\n",
    "# y_pred = logreg.predict_proba(X_test)\n",
    "# pd.DataFrame(y_pred,columns=le.classes_)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check predictions\n",
    "\n",
    "# print(df_pred.shape)\n",
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['svc','rfc','dtc','lr','knn']\n",
    "# df_pred = df_pred[cols]\n",
    "\n",
    "# df_pred_mode = df_pred.mode(axis=1)\n",
    "# df_pred_mode.reset_index(inplace=True)\n",
    "\n",
    "# df_pred_final = df_pred_mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx,row in df_pred_mode.iterrows():\n",
    "    \n",
    "#     if np.isnan(row[1]) == False:\n",
    "\n",
    "#         mode_list = list(row[1:])\n",
    "#         #print(mode_list)\n",
    "        \n",
    "#         pred_row = df_pred.iloc[idx,:]\n",
    "#         #print(pred_row)\n",
    "        \n",
    "        \n",
    "#         for idx_2,cell in pred_row.iteritems():\n",
    "            \n",
    "#             if cell in mode_list:\n",
    "#                 #print(idx,idx_2,cell)\n",
    "#                 df_pred_final[idx] = cell\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confusion matrix\n",
    "# print('Combined Model')\n",
    "# print('rows:actual columns:predicted')\n",
    "\n",
    "# y_pred_final = df_pred_final.values\n",
    "\n",
    "# f1 = f1_score(y_test, y_pred,average='micro')\n",
    "# accuracy = accuracy_score(y_test,y_pred)\n",
    "# print('f1_score:',f1)\n",
    "# print('accuracy_score:',accuracy)\n",
    "\n",
    "# # cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# # print('cohen_score',cohen_score)\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred_final)\n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "dtc = DecisionTreeClassifier(max_depth=10,random_state=3050)\n",
    "rfc = RandomForestClassifier(n_estimators=10,max_depth=10,n_jobs=-1, random_state=3050)\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr knn dtc rfc svc\n",
    "\n",
    "estimators = [\n",
    "        ('lr', lr), \n",
    "        ('knn', knn), \n",
    "        ('dtc', dtc),\n",
    "        ('rfc', rfc),\n",
    "        ('svc', svc)\n",
    "            ]\n",
    "\n",
    "vc = VotingClassifier(estimators=estimators, voting='hard')\n",
    "#vc = VotingClassifier(estimators=estimators, voting='soft')\n",
    "vc = vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print('VC Model')\n",
    "print('rows:actual columns:predicted')\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "precision = precision_score(y_test,y_pred,average='weighted')\n",
    "recall = recall_score(y_test,y_pred,average='weighted')\n",
    "print('accuracy_score:',accuracy)\n",
    "print('f1_score:',f1)\n",
    "print('precision_score:',precision)\n",
    "print('recall_score:',recall)\n",
    "\n",
    "# cohen_score = cohen_kappa_score(y_test, y_pred)\n",
    "# print('cohen_score',cohen_score)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(data=cm, columns=le.classes_, index=le.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_cm,annot=True,cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "\n",
    "estimators = {\n",
    "    'lr': LogisticRegression(),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    'abc': AdaBoostClassifier(),\n",
    "    'gbc': GradientBoostingClassifier()\n",
    "}.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model parameters\n",
    "\n",
    "params = {\n",
    "    'lr': {\n",
    "        'lr__penalty': ['l1','l2'],\n",
    "        #'lr__penalty': ['l1','l2','elasticnet'],\n",
    "        #'l1__ratio': np.arange(.1, 1, .2)\n",
    "    },\n",
    "    'knn': {\n",
    "        'knn__n_neighbors': [3,5,7,9],\n",
    "        'knn__weights': ['uniform','distance']\n",
    "    },\n",
    "\n",
    "    'dtc': {\n",
    "        'dtc__max_features': ['auto', 'log2', None],\n",
    "        'dtc__max_depth': np.arange(3, 16, 2),\n",
    "        'dtc__min_samples_split': np.linspace(0.1, 0.5, 5)\n",
    "    },\n",
    "    'rfc': {\n",
    "        'rfc__n_estimators': [10, 15, 20, 25],\n",
    "        'rfc__max_features': ['auto', 'log2', None],\n",
    "        'rfc__max_depth': np.arange(3, 16, 2),\n",
    "        'rfc__min_samples_split': np.linspace(0.1, 0.5, 5)\n",
    "    },\n",
    "    'abc': {\n",
    "        'abc__n_estimators' : np.arange(50, 151, 25),\n",
    "        'abc__learning_rate' : np.linspace(0.1, 1, 8)\n",
    "    }, \n",
    "    'gbc': {\n",
    "        'gbc__n_estimators' : np.arange(10, 101, 30),\n",
    "        'gbc__learning_rate' : np.linspace(0.1, 1, 4),\n",
    "        'gbc__max_depth' : [1, 2, 3]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "parameters = []\n",
    "best_score = []\n",
    "roc_auc = []\n",
    "\n",
    "for k,v in estimators:\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            #('sc', StandardScaler()),\n",
    "            (k,v)\n",
    "                    ])\n",
    "    \n",
    "    gridsearch = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=params[k],\n",
    "        verbose=1,\n",
    "        cv= 5,\n",
    "        n_jobs=-1,\n",
    "        return_train_score= True\n",
    "    )\n",
    "\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    \n",
    "    model = gridsearch.best_estimator_\n",
    "    cv_score = gridsearch.cv_results_\n",
    "    best_params = gridsearch.best_params_\n",
    "\n",
    "    # predict y\n",
    "    y_pred = model.predict(X_test)\n",
    "    #y_pred = model.predict_proba(X_test)\n",
    "    \n",
    "    # print results\n",
    "    print(\"Model: \", k)\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best score:\", gridsearch.best_score_)\n",
    "    display(pd.DataFrame(cv_score, columns = cv_score.keys()))    \n",
    "    \n",
    "    # append info to list\n",
    "    models.append(k)\n",
    "    best_score.append(gridsearch.best_score_)\n",
    "    parameters.append(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output gridsearch results\n",
    "\n",
    "print(models)\n",
    "print(best_score)\n",
    "print(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
